{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQzZwiH+A/qNUtaWdkVfGm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gagalin105/TAHRD2nd-PL/blob/main/HW4%EF%BC%9A%E7%88%AC%E8%9F%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **原程式碼**\n",
        "此格不執行"
      ],
      "metadata": {
        "id": "wJP96SJdIU7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgzzc-ohZv9f"
      },
      "outputs": [],
      "source": [
        "#本來使用老師教的方法，但是發現沒辦法執行出結果，上網查+詢問AI以後才得知有使用javascript編成的網站無法直接使用python編譯\n",
        "from selenium import webdriver #網站使用javascript，必須使用不同模擬瀏覽器\n",
        "\n",
        "from bs4 import BeautifulSoup#import BeautifulSoup\n",
        "driver = webdriver.Chrome()  # 確保已安裝 ChromeDriver\n",
        "\n",
        "#cookie:__Secure-3PSID=g.a000qwjoFOmpa05rJShJiyuGzvqL6KrgVCjFgiqyWpg5eTNZFQmxCk8z1AAlCkchJIr6exT-TAACgYKAfUSARASFQHGX2Mi0EZBvNnKI0VB0ScNl_j4LhoVAUF8yKo1akDVXIYIjCgqZwI74NnH007\n",
        "#user-agent:Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1\n",
        "\n",
        "#寫入第一頁標題\n",
        "driver.get(\"https://forum.gamer.com.tw/B.php?page=1&bsn=7731\")\n",
        "import time\n",
        "time.sleep(3) #等待頁面載入完成\n",
        "html = driver.page_source\n",
        "\n",
        "with open(\"cr.html\",\"wt\",encoding=\"utf-8\")as ftext: #測試抓程式\n",
        "    print(html,file=ftext)\n",
        "soup = BeautifulSoup(html, \"html.parser\") # 使用 BeautifulSoup 解析 HTML\n",
        "div_tags = soup.find_all(\"p\", {\"class\": \"b-list__main__title\"})\n",
        "print(f\"第1頁找到 {len(div_tags)} 個標題\")\n",
        "#寫入標題\n",
        "with open(\"yu.txt\",\"a\",encoding=\"utf-8\")as ftext: #為使檔案不被覆蓋，使用\"a\"來寫入\n",
        "    for div_tag in div_tags:\n",
        "        ftext.write(div_tag.text.strip()+\"\\n\")\n",
        "\n",
        "#重複寫入2-11頁(每頁共30行)\n",
        "for n in range (2,12):\n",
        "    driver.get(f\"https://forum.gamer.com.tw/B.php?page={n}&bsn=7731\")\n",
        "    import time\n",
        "    time.sleep(3)\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    div_tags = soup.find_all(\"p\", {\"class\": \"b-list__main__title\"})\n",
        "    print(f\"第{n}頁找到 {len(div_tags)} 個標題\")\n",
        "    with open(\"yu.txt\",\"a\",encoding=\"utf-8\")as ftext:\n",
        "        for div_tag in div_tags:\n",
        "            ftext.write(div_tag.text.strip()+\"\\n\")\n",
        "driver.quit() #結束載入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dbb98ef"
      },
      "source": [
        "# Task\n",
        "修改筆記本中的程式碼，將抓取的標題寫入 Google Sheets 試算表 \"https://docs.google.com/spreadsheets/d/1GTsjiStggnSWJRGtwC0JRngF4wQAxvlyiGT8xFqSvNA/edit?usp=sharing\" 中，而不是寫入本地檔案。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages for headless Chrome\n",
        "!pip install selenium webdriver-manager\n",
        "!apt-get update\n",
        "!apt-get install -y libglib2.0-0 libnss3 libfontconfig1\n",
        "\n",
        "# Install Chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -f # Install dependencies if any"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7L_8syEfKiE",
        "outputId": "9f6ae859-63c9-4518-db65-ea2b08f25be7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
            "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.10.5 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.10.5)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
            "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->webdriver-manager) (3.4.4)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.38.0 trio-0.32.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n",
            "Hit:1 https://dl.google.com/linux/chrome/deb stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://cli.github.com/packages stable InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.6).\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "--2025-11-03 06:56:44--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 173.194.216.190, 173.194.216.93, 173.194.216.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|173.194.216.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117592096 (112M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb.2’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 112.14M   300MB/s    in 0.4s    \n",
            "\n",
            "2025-11-03 06:56:44 (300 MB/s) - ‘google-chrome-stable_current_amd64.deb.2’ saved [117592096/117592096]\n",
            "\n",
            "(Reading database ... 125399 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (142.0.7444.59-1) over (142.0.7444.59-1) ...\n",
            "Setting up google-chrome-stable (142.0.7444.59-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaYsjdccDnIr",
        "outputId": "c2829b56-1ac4-4329-cf84-33d596296748"
      },
      "source": [
        "# Define the URL of the target Google Sheet\n",
        "sheet_url = \"https://docs.google.com/spreadsheets/d/1GTsjiStggnSWJRGtwC0JRngF4wQAxvlyiGT8xFqSvNA/edit?usp=sharing\"\n",
        "\n",
        "# Open the Google Sheet using the gspread client\n",
        "try:\n",
        "    sh = gc.open_by_url(sheet_url)\n",
        "    # Select the specific worksheet (usually the first one)\n",
        "    worksheet = sh.sheet1\n",
        "except Exception as e:\n",
        "    print(f\"Error opening Google Sheet: {e}\")\n",
        "    # Handle the error, maybe finish the task with failure\n",
        "    # For now, let's assume it's a critical error and exit\n",
        "    raise\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Configure Chrome options for headless execution and specify the binary location\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.binary_location = \"/opt/google/chrome/google-chrome\" # Update Chrome binary location\n",
        "\n",
        "# Initialize the Chrome driver using webdriver_manager and Service\n",
        "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "\n",
        "# Remove or comment out the code related to opening and writing to the local file \"yu.txt\"\n",
        "\n",
        "def scrape_and_write_page(page_num, driver, worksheet):\n",
        "    \"\"\"Scrapes titles from a single page and writes them to the Google Sheet.\"\"\"\n",
        "    url = f\"https://forum.gamer.com.tw/B.php?page={page_num}&bsn=7731\"\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    div_tags = soup.find_all(\"p\", {\"class\": \"b-list__main__title\"})\n",
        "    print(f\"第{page_num}頁找到 {len(div_tags)} 個標題\")\n",
        "\n",
        "    # Append each title as a new row to the selected worksheet\n",
        "    for div_tag in div_tags:\n",
        "        try:\n",
        "            worksheet.append_row([div_tag.text.strip()])\n",
        "        except Exception as e:\n",
        "            print(f\"Error appending row to Google Sheet: {e}\")\n",
        "            # Optionally, log the failed row or handle differently\n",
        "\n",
        "# Write titles from page 1 to 11\n",
        "for n in range(1, 12):\n",
        "    scrape_and_write_page(n, driver, worksheet)\n",
        "\n",
        "\n",
        "driver.quit() # Close the browser instance"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第1頁找到 30 個標題\n",
            "第2頁找到 30 個標題\n",
            "第3頁找到 30 個標題\n",
            "第4頁找到 30 個標題\n",
            "第5頁找到 30 個標題\n",
            "第6頁找到 30 個標題\n",
            "第7頁找到 30 個標題\n",
            "第8頁找到 30 個標題\n",
            "第9頁找到 30 個標題\n",
            "第10頁找到 30 個標題\n",
            "第11頁找到 30 個標題\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ec2c472"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The necessary libraries for interacting with Google Sheets (`gspread` and `oauth2client`) were already installed.\n",
        "*   Google Colab authentication was successfully configured using default user credentials, authorizing the `gspread` client to access Google Sheets.\n",
        "*   Attempts to install and configure a headless browser (Chromium) and its driver using `apt-get` commands for web scraping with Selenium failed repeatedly.\n",
        "*   Due to the failure in browser installation, the web scraping part of the task using Selenium could not be executed, preventing the script from extracting titles from the website.\n",
        "*   Although the Google Sheet was successfully opened using `gspread`, no data could be written to it because the preceding scraping step failed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Future attempts to run this code in a similar environment should first verify the available browser packages and the correct installation commands or consider using a different scraping method that does not rely on a full browser instance (e.g., using `requests` and `BeautifulSoup` if the content is available in the initial HTML).\n",
        "*   Ensure the correct binary location for the browser is specified in the Selenium options based on the successful installation path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "119837b1"
      },
      "source": [
        "# Task\n",
        "將筆記本中的程式碼修改為將結果輸出到 Google Sheet \"https://docs.google.com/spreadsheets/d/1GTsjiStggnSWJRGtwC0JRngF4wQAxvlyiGT8xFqSvNA/edit?usp=sharing\" 中，並將重複的程式碼進行整理，最後將整個流程整合成 Gradio 介面。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "272cc254"
      },
      "source": [
        "## 從 google sheet 讀取資料\n",
        "\n",
        "### Subtask:\n",
        "使用 `gspread` 從指定的 Google Sheet 中讀取資料。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1ea682"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries for interacting with Google Sheets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac246b01",
        "outputId": "84f962c8-ba44-4937-91cf-f69691c46df3"
      },
      "source": [
        "%pip install gspread oauth2client"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.12/dist-packages (4.1.3)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client) (0.31.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.12.0->gspread) (5.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bce14d4"
      },
      "source": [
        "**Reasoning**:\n",
        "Authorize gspread using Google Colab's default credentials and read data from the specified Google Sheet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae519260",
        "outputId": "ca31abf7-64d8-474c-c2ec-bfff07d92aa2"
      },
      "source": [
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default # Import default from google.auth\n",
        "\n",
        "# Authenticate with Google Colab\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get the default credentials using google.auth.default()\n",
        "creds, project = default()\n",
        "\n",
        "# Authorize gspread with the obtained credentials\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "\n",
        "# Define the URL of the target Google Sheet\n",
        "sheet_url = \"https://docs.google.com/spreadsheets/d/1GTsjiStggnSWJRGtwC0JRngF4wQAxvlyiGT8xFqSvNA/edit?usp=sharing\"\n",
        "\n",
        "# Open the Google Sheet by URL\n",
        "try:\n",
        "    sh = gc.open_by_url(sheet_url)\n",
        "    # Select the first worksheet\n",
        "    worksheet = sh.sheet1\n",
        "\n",
        "    # Read all values from the worksheet\n",
        "    sheet_data = worksheet.get_all_values()\n",
        "\n",
        "    # Print the first few rows of the data to verify\n",
        "    print(\"Successfully read data from Google Sheet:\")\n",
        "    for row in sheet_data[:5]:\n",
        "        print(row)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Google Sheet: {e}\")\n",
        "    sheet_data = None"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read data from Google Sheet:\n",
            "['【情報】魂魄覺醒！和尚參戰']\n",
            "['【情報】動畫《BLEACH 死神 》 馬斯科·多·馬斯庫林 10/20 生日祝賀(2025)']\n",
            "['【情報】BANDAI SPIRITS Grandista-黑崎一護-']\n",
            "['【情報】《BLEACH 死神 千年血戰篇》× 大丸松坂屋限定快閃店 10/22 東京、11/5 梅田開幕！']\n",
            "['【討論】小說看來有帶動銷量']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69adadb8"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates an issue with the credentials type. The `gspread.authorize` method expects credentials from either `oauth2client` or `google-auth`. Based on the provided code, it seems that the `GoogleCredentials.get_application_default()` is returning a credential type that is not compatible with the latest `gspread` version. I will try using credentials from `google.auth` which is the recommended way now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSUK0bgDQiSP",
        "outputId": "0fa465b0-0532-4568-a9be-af87e84e2aef"
      },
      "source": [
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "# Authenticate with Google Colab\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get the default credentials\n",
        "creds, project = default()\n",
        "\n",
        "# Authorize gspread with the obtained credentials\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "\n",
        "# Define the URL of the target Google Sheet\n",
        "sheet_url = \"https://docs.google.com/spreadsheets/d/1GTsjiStggnSWJRGtwC0JRngF4wQAxvlyiGT8xFqSvNA/edit?usp=sharing\"\n",
        "\n",
        "# Open the Google Sheet by URL\n",
        "try:\n",
        "    sh = gc.open_by_url(sheet_url)\n",
        "    # Select the first worksheet\n",
        "    worksheet = sh.sheet1\n",
        "\n",
        "    # Read all values from the worksheet\n",
        "    sheet_data = worksheet.get_all_values()\n",
        "\n",
        "    # Print the first few rows of the data to verify\n",
        "    print(\"Successfully read data from Google Sheet:\")\n",
        "    for row in sheet_data[:5]:\n",
        "        print(row)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Google Sheet: {e}\")\n",
        "    sheet_data = None"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully read data from Google Sheet:\n",
            "['【情報】魂魄覺醒！和尚參戰']\n",
            "['【情報】動畫《BLEACH 死神 》 馬斯科·多·馬斯庫林 10/20 生日祝賀(2025)']\n",
            "['【情報】BANDAI SPIRITS Grandista-黑崎一護-']\n",
            "['【情報】《BLEACH 死神 千年血戰篇》× 大丸松坂屋限定快閃店 10/22 東京、11/5 梅田開幕！']\n",
            "['【討論】小說看來有帶動銷量']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58fd11c2"
      },
      "source": [
        "## 詞頻與關鍵字統計（tf-idf）\n",
        "\n",
        "### Subtask:\n",
        "對讀取的資料進行文字前處理（如：分詞、去除停用詞），然後計算詞頻（TF）和逆文件頻率（IDF），最後計算 TF-IDF 值。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9420e82"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries for text processing, specifically jieba for Chinese word segmentation and scikit-learn for TF-IDF calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "300aa386",
        "outputId": "5953d156-a5d8-4838-8bba-1dcc9b24c09b"
      },
      "source": [
        "%pip install jieba scikit-learn"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "806c726e"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function for text preprocessing including Chinese word segmentation and stop word removal, and then apply it to the data read from the Google Sheet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad19fabc",
        "outputId": "39665333-81c7-4f37-a920-dbc621107015"
      },
      "source": [
        "import jieba\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define a simple list of stop words (can be expanded)\n",
        "stopwords = set([\n",
        "    '的', '是', '在', '不', '了', '有', '和', '以', '等', '之', '與', '於',\n",
        "    '這', '那', '個', '人', '們', '可以', '進行', '使用', '一個', '一些', '各種'\n",
        "])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Performs Chinese word segmentation and removes stop words.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    # Segment the text\n",
        "    seg_list = jieba.cut(text, cut_all=False)\n",
        "    # Filter out stop words\n",
        "    processed_words = [word for word in seg_list if word.strip() and word not in stopwords]\n",
        "    return processed_words\n",
        "\n",
        "# Apply the preprocessing function to each title in the sheet_data\n",
        "# Assuming titles are in the first column (index 0) and skipping the header row if it exists\n",
        "processed_titles = [preprocess_text(row[0]) for row in sheet_data[1:]] # Adjust slicing if header is not present or titles are in a different column\n",
        "\n",
        "# Join the processed words back into strings for TF-IDF calculation\n",
        "processed_titles_joined = [\" \".join(words) for words in processed_titles]\n",
        "\n",
        "print(f\"Processed the first 5 titles: {processed_titles[:5]}\")\n",
        "print(f\"Joined processed titles for TF-IDF (first 5): {processed_titles_joined[:5]}\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed the first 5 titles: [['【', '情報', '】', '動畫', '《', 'BLEACH', '死神', '》', '馬', '斯科', '·', '多', '·', '馬', '斯庫林', '10', '/', '20', '生日', '祝賀', '(', '2025', ')'], ['【', '情報', '】', 'BANDAI', 'SPIRITS', 'Grandista', '-', '黑崎一護', '-'], ['【', '情報', '】', '《', 'BLEACH', '死神', '千年', '血戰篇', '》', '×', '大丸', '松坂屋', '限定', '快', '閃店', '10', '/', '22', '東京', '、', '11', '/', '5', '梅田', '開幕', '！'], ['【', '討論', '】', '小說', '看來', '帶動', '銷量'], ['【', '情報', '】', '萬代', '《', 'BLEACH', '死神', '》', '轉蛋', '吊飾', '藍染', '惣', '右介', '，', '預計', '10', '月', '第四', '週起', '推出', '！']]\n",
            "Joined processed titles for TF-IDF (first 5): ['【 情報 】 動畫 《 BLEACH 死神 》 馬 斯科 · 多 · 馬 斯庫林 10 / 20 生日 祝賀 ( 2025 )', '【 情報 】 BANDAI SPIRITS Grandista - 黑崎一護 -', '【 情報 】 《 BLEACH 死神 千年 血戰篇 》 × 大丸 松坂屋 限定 快 閃店 10 / 22 東京 、 11 / 5 梅田 開幕 ！', '【 討論 】 小說 看來 帶動 銷量', '【 情報 】 萬代 《 BLEACH 死神 》 轉蛋 吊飾 藍染 惣 右介 ， 預計 10 月 第四 週起 推出 ！']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "312c5a46"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a TfidfVectorizer object and calculate the TF-IDF values for the processed titles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-UMQx1KS4be",
        "outputId": "7e80876e-b608-497d-f2d0-f49a598db9fa"
      },
      "source": [
        "# Initialize TfidfVectorizer\n",
        "# Set tokenizer, preprocessor, and token_pattern to None as we have already preprocessed and tokenized the text\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, token_pattern=None)\n",
        "\n",
        "\n",
        "# Calculate TF-IDF values\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_titles_joined)\n",
        "\n",
        "# The tfidf_matrix is a sparse matrix. You can convert it to a dense array if needed.\n",
        "# tfidf_matrix_dense = tfidf_matrix.todense()\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
        "# print(\"TF-IDF matrix (dense, first 5 rows):\", tfidf_matrix_dense[:5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape: (989, 1191)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c54f99"
      },
      "source": [
        "## 輸出前 n 熱詞\n",
        "\n",
        "### Subtask:\n",
        "根據 TF-IDF 值，找出前 N 個熱門詞彙。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf958cad"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate the importance of each term based on their TF-IDF values across all documents and then identify the top N terms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed88022c",
        "outputId": "3d9a1b1d-888a-4d35-deaf-6d26fd619f39"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the feature names (words) from the vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Calculate the sum of TF-IDF scores for each term across all documents\n",
        "# Since tfidf_matrix is a sparse matrix, sum along axis 0\n",
        "term_tfidf_sums = np.sum(tfidf_matrix, axis=0)\n",
        "\n",
        "# Convert the result to a dense array\n",
        "term_tfidf_sums = np.asarray(term_tfidf_sums).flatten()\n",
        "\n",
        "# Create a dictionary mapping each term to its total TF-IDF sum\n",
        "term_importance = dict(zip(feature_names, term_tfidf_sums))\n",
        "\n",
        "# Sort the terms by their importance in descending order\n",
        "sorted_terms = sorted(term_importance.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Define the number of top terms to retrieve (e.g., 10)\n",
        "N = 10\n",
        "top_n_terms = sorted_terms[:N]\n",
        "\n",
        "print(f\"Top {N} most important terms based on TF-IDF:\")\n",
        "for term, score in top_n_terms:\n",
        "    print(f\"{term}: {score:.4f}\")\n",
        "\n",
        "# Store the top N terms in a variable for later use\n",
        "top_terms_list = [term for term, score in top_n_terms]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most important terms based on TF-IDF:\n",
            "【: 68.3925\n",
            "】: 68.3925\n",
            "死神: 53.7196\n",
            "BLEACH: 52.3228\n",
            "情報: 51.9388\n",
            "-: 50.4470\n",
            "《: 50.1908\n",
            "》: 50.1908\n",
            "/: 48.3494\n",
            "動畫: 41.2601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ce975e"
      },
      "source": [
        "## 回寫到統計表\n",
        "\n",
        "### Subtask:\n",
        "將前 N 熱詞及相關統計數據寫回 Google Sheet 的另一個工作表中。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698f9e38"
      },
      "source": [
        "**Reasoning**:\n",
        "Attempt to open or create the \"TF-IDF_Stats\" worksheet, clear its contents, write the header row, and then iterate through the top terms to write them into the sheet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c20dfd0c",
        "outputId": "a0f389a0-2291-4911-dd36-b816ca8e5f6f"
      },
      "source": [
        "try:\n",
        "    # Attempt to open the \"TF-IDF_Stats\" worksheet\n",
        "    stats_worksheet = sh.worksheet(\"TF-IDF_Stats\")\n",
        "    print(\"Opened existing 'TF-IDF_Stats' worksheet.\")\n",
        "except gspread.WorksheetNotFound:\n",
        "    # If the worksheet doesn't exist, create it\n",
        "    stats_worksheet = sh.add_worksheet(title=\"TF-IDF_Stats\", rows=\"100\", cols=\"20\")\n",
        "    print(\"Created new 'TF-IDF_Stats' worksheet.\")\n",
        "\n",
        "# Clear the existing contents of the worksheet\n",
        "stats_worksheet.clear()\n",
        "print(\"'TF-IDF_Stats' worksheet cleared.\")\n",
        "\n",
        "# Write the header row\n",
        "header = ['Term', 'Total TF-IDF Score']\n",
        "stats_worksheet.append_row(header)\n",
        "print(\"Header row written.\")\n",
        "\n",
        "# Write the top N terms and their scores to the worksheet\n",
        "for term, score in top_n_terms:\n",
        "    try:\n",
        "        stats_worksheet.append_row([term, score])\n",
        "    except Exception as e:\n",
        "        print(f\"Error appending row for term '{term}': {e}\")\n",
        "\n",
        "print(\"Top N terms and TF-IDF scores successfully written to Google Sheet.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opened existing 'TF-IDF_Stats' worksheet.\n",
            "'TF-IDF_Stats' worksheet cleared.\n",
            "Header row written.\n",
            "Top N terms and TF-IDF scores successfully written to Google Sheet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e859a856"
      },
      "source": [
        "## 串接 gemini api 生成摘要與結論\n",
        "\n",
        "### Subtask:\n",
        "使用 Gemini API，根據抓取的標題和詞頻統計結果，生成 5 句洞察摘要和一段 120 字的結論。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f75df73"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for the Gemini API and authenticate. Then, prepare the prompt with the relevant data and call the Gemini API to generate the summary and conclusion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "b4c2547f",
        "outputId": "feda9359-f113-4feb-a7d2-7fcc96538c98"
      },
      "source": [
        "# Ensure the Google Generative AI library is installed\n",
        "%pip install google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the Gemini API key\n",
        "# It's assumed the API key is stored as a user secret in Colab\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_SERVICE_ACCOUNT_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Prepare the prompt\n",
        "prompt_text = f\"\"\"\n",
        "Based on the following data which includes a list of titles and the importance scores of terms found within those titles, provide a summary and a conclusion.\n",
        "\n",
        "Titles:\n",
        "{sheet_data}\n",
        "\n",
        "Term Importance (Top {N} terms based on TF-IDF):\n",
        "{top_n_terms}\n",
        "\n",
        "Please generate:\n",
        "1. Five insightful bullet points summarizing key observations from the titles and term importance.\n",
        "2. A conclusion of approximately 120 words that synthesizes the main trends or focus areas of the titles based on the provided data.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the generative model\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# Generate the response\n",
        "try:\n",
        "    response = model.generate_content(prompt_text)\n",
        "    # Access the generated text\n",
        "    generated_text = response.text\n",
        "\n",
        "    # Split the generated text into summary and conclusion\n",
        "    # Assuming the model separates the summary and conclusion clearly, e.g., with headings\n",
        "    summary_heading = \"1.\"\n",
        "    conclusion_heading = \"2.\"\n",
        "\n",
        "    if summary_heading in generated_text and conclusion_heading in generated_text:\n",
        "        summary_start = generated_text.find(summary_heading)\n",
        "        conclusion_start = generated_text.find(conclusion_heading)\n",
        "\n",
        "        summary = generated_text[summary_start:conclusion_start].strip()\n",
        "        conclusion = generated_text[conclusion_start:].strip()\n",
        "    else:\n",
        "        # If headings are not found, assign the whole text or parts based on assumptions\n",
        "        # For simplicity, let's assume the summary comes first then the conclusion\n",
        "        # A more robust parsing might be needed based on actual model output format\n",
        "        print(\"Warning: Could not find expected headings in generated text. Attempting simple split.\")\n",
        "        lines = generated_text.split('\\n')\n",
        "        summary_lines = []\n",
        "        conclusion_lines = []\n",
        "        is_conclusion = False\n",
        "        for line in lines:\n",
        "            if \"conclusion\" in line.lower() or \"synthesize\" in line.lower() or \"總結\" in line: # Basic check for conclusion indicator\n",
        "                 is_conclusion = True\n",
        "            if not is_conclusion:\n",
        "                summary_lines.append(line)\n",
        "            else:\n",
        "                 conclusion_lines.append(line)\n",
        "        summary = \"\\n\".join(summary_lines).strip()\n",
        "        conclusion = \"\\n\".join(conclusion_lines).strip()\n",
        "\n",
        "\n",
        "    # Store the generated summary and conclusion in variables\n",
        "    generated_summary = summary\n",
        "    generated_conclusion = conclusion\n",
        "\n",
        "    print(\"Generated Summary:\")\n",
        "    print(generated_summary)\n",
        "    print(\"\\nGenerated Conclusion:\")\n",
        "    print(generated_conclusion)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error generating content with Gemini API: {e}\")\n",
        "    generated_summary = \"Error generating summary.\"\n",
        "    generated_conclusion = \"Error generating conclusion.\"\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.185.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1019.80ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating content with Gemini API: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87630ee8"
      },
      "source": [
        "## 整合成 gradio 介面\n",
        "\n",
        "### Subtask:\n",
        "建立一個 Gradio 介面，讓使用者可以一鍵執行整個流程並顯示結果（熱詞、摘要、結論）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6fe41a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the gradio library and define the run_analysis function to encapsulate the entire process, including reading data, preprocessing, TF-IDF, generating summary and conclusion, and writing to the sheet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "08bbd1b9",
        "outputId": "6ab73398-f239-4e44-9ebc-95edc31d1b6e"
      },
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "\n",
        "# Authenticate with Google Colab (This part needs to be handled outside the function\n",
        "# or in a way compatible with Gradio. For Colab environment, it's typically done once.)\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, project = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"Google Colab authentication successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Google Colab authentication failed: {e}\")\n",
        "    gc = None # Set gc to None if authentication fails\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Segments Chinese text and removes stop words.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    seg_list = jieba.cut(text, cut_all=False)\n",
        "    # Add common stop words, including punctuation that might not be removed by default\n",
        "    stop_words = set(['的', '是', '在', '我', '有', '了', '和', '就', '不', '人', '都', '一', '也', '很', '會', '你', '到', '想', '知', '更', '再', '沒', '看', '來', '要', '可', '行', '能', '說', '個', '這', '那', '用', '只', '比', '些', '從', '去', '先', '把', '給', '對', ':', '：', ',', '，', '.', '。', '!', '！', '?', '？', ';', '；', '\"', '“', '”', \"'\", '‘', '’', '(', '（', ')', '）', '[', '【', ']', '】', '{', '}', '<', '>', '/', '\\\\', '|', '@', '#', '$', '%', '^', '&', '*', '_', '-', '+', '=', '~', '`', '...', '....', '.....', '......']) # Extended stop words\n",
        "    return \" \".join([word for word in seg_list if word not in stop_words and word.strip()])\n",
        "\n",
        "\n",
        "def run_analysis(sheet_url):\n",
        "    \"\"\"\n",
        "    Reads data from Google Sheet, performs TF-IDF, finds top terms,\n",
        "    generates summary and conclusion, and writes to the sheet.\n",
        "    \"\"\"\n",
        "    if gc is None:\n",
        "        return \"Google Sheet access not authorized. Please ensure authentication is set up correctly.\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        sh = gc.open_by_url(sheet_url)\n",
        "        worksheet = sh.sheet1\n",
        "        sheet_data = worksheet.get_all_values()\n",
        "        if not sheet_data:\n",
        "            return \"No data found in the Google Sheet.\", \"\", \"\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading from Google Sheet: {e}\", \"\", \"\"\n",
        "\n",
        "    # Assuming the data is in the first column after headers, if any\n",
        "    # Let's assume the first row is header and actual titles start from the second row\n",
        "    if len(sheet_data) > 1:\n",
        "        titles = [row[0] for row in sheet_data[1:] if row] # Extract titles from the first column, skip empty rows\n",
        "    else:\n",
        "         return \"No title data found after header row.\", \"\", \"\"\n",
        "\n",
        "\n",
        "    # Preprocess titles\n",
        "    processed_titles = [preprocess_text(title) for title in titles]\n",
        "    processed_titles_joined = [title for title in processed_titles if title.strip()] # Filter out empty strings after preprocessing\n",
        "\n",
        "    if not processed_titles_joined:\n",
        "         return \"No valid text found after preprocessing.\", \"\", \"\"\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, token_pattern=None)\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(processed_titles_joined)\n",
        "    except ValueError as e:\n",
        "        return f\"Error calculating TF-IDF. Ensure there is enough text data to build vocabulary: {e}\", \"\", \"\"\n",
        "\n",
        "\n",
        "    # Find top N terms\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    term_tfidf_sums = np.sum(tfidf_matrix, axis=0)\n",
        "    term_tfidf_sums = np.asarray(term_tfidf_sums).flatten()\n",
        "    term_importance = dict(zip(feature_names, term_tfidf_sums))\n",
        "    sorted_terms = sorted(term_importance.items(), key=lambda item: item[1], reverse=True)\n",
        "    N = 10  # Define number of top terms\n",
        "    top_n_terms = sorted_terms[:N]\n",
        "    top_terms_list = [term for term, score in top_n_terms]\n",
        "\n",
        "    # Write to TF-IDF_Stats worksheet\n",
        "    try:\n",
        "        try:\n",
        "            stats_worksheet = sh.worksheet(\"TF-IDF_Stats\")\n",
        "        except gspread.WorksheetNotFound:\n",
        "            stats_worksheet = sh.add_worksheet(title=\"TF-IDF_Stats\", rows=\"100\", cols=\"20\")\n",
        "\n",
        "        stats_worksheet.clear()\n",
        "        header = ['Term', 'Total TF-IDF Score']\n",
        "        stats_worksheet.append_row(header)\n",
        "        for term, score in top_n_terms:\n",
        "            stats_worksheet.append_row([term, score])\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to TF-IDF_Stats worksheet: {e}\") # Print error but don't stop the process\n",
        "\n",
        "\n",
        "    # Configure the Gemini API key\n",
        "    # It's assumed the API key is stored as a user secret in Colab\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_SERVICE_ACCOUNT_KEY')\n",
        "    if not GOOGLE_API_KEY:\n",
        "        generated_summary = \"Gemini API key not found. Cannot generate summary and conclusion.\"\n",
        "        generated_conclusion = \"Gemini API key not found. Cannot generate summary and conclusion.\"\n",
        "    else:\n",
        "        try:\n",
        "            genai.configure(api_key=GOOGLE_API_KEY)\n",
        "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "            # Prepare the prompt for Gemini\n",
        "            prompt_text = f\"\"\"\n",
        "            Based on the following list of titles from a forum and the top {N} most important terms found within them, provide a summary and a conclusion.\n",
        "\n",
        "            Titles (first 10 examples):\n",
        "            {titles[:10]}\n",
        "\n",
        "            Top {N} most important terms based on TF-IDF:\n",
        "            {top_n_terms}\n",
        "\n",
        "            Please generate:\n",
        "            1. Five insightful bullet points summarizing key observations about the topics and trends discussed in the titles based on the provided data.\n",
        "            2. A conclusion of approximately 120 words that synthesizes the main focus areas and overall sentiment (if discernible) of the forum content, referencing the important terms.\n",
        "            \"\"\"\n",
        "\n",
        "            response = model.generate_content(prompt_text)\n",
        "            generated_text = response.text\n",
        "\n",
        "            # Basic parsing of the generated text\n",
        "            summary_heading = \"1.\"\n",
        "            conclusion_heading = \"2.\"\n",
        "\n",
        "            if summary_heading in generated_text and conclusion_heading in generated_text:\n",
        "                summary_start = generated_text.find(summary_heading)\n",
        "                conclusion_start = generated_text.find(conclusion_heading)\n",
        "\n",
        "                summary = generated_text[summary_start:conclusion_start].strip()\n",
        "                conclusion = generated_text[conclusion_start:].strip()\n",
        "            else:\n",
        "                 # Fallback parsing if headings aren't found as expected\n",
        "                lines = generated_text.split('\\n')\n",
        "                summary_lines = []\n",
        "                conclusion_lines = []\n",
        "                is_conclusion = False\n",
        "                for line in lines:\n",
        "                    if \"conclusion\" in line.lower() or \"synthesize\" in line.lower() or \"總結\" in line:\n",
        "                         is_conclusion = True\n",
        "                    if not is_conclusion:\n",
        "                        summary_lines.append(line)\n",
        "                    else:\n",
        "                         conclusion_lines.append(line)\n",
        "                summary = \"\\n\".join(summary_lines).strip()\n",
        "                conclusion = \"\\n\".join(conclusion_lines).strip()\n",
        "\n",
        "\n",
        "            generated_summary = summary\n",
        "            generated_conclusion = conclusion\n",
        "\n",
        "        except Exception as e:\n",
        "            generated_summary = f\"Error generating summary/conclusion with Gemini API: {e}\"\n",
        "            generated_conclusion = f\"Error generating summary/conclusion with Gemini API: {e}\"\n",
        "\n",
        "\n",
        "    # Format top terms for display\n",
        "    top_terms_display = \"\\n\".join([f\"{term}: {score:.4f}\" for term, score in top_n_terms])\n",
        "\n",
        "    return top_terms_display, generated_summary, generated_conclusion\n",
        "\n",
        "# Create the Gradio interface\n",
        "if gc is not None: # Only create interface if authentication was successful\n",
        "    interface = gr.Interface(\n",
        "        fn=run_analysis,\n",
        "        inputs=gr.Textbox(label=\"Enter Google Sheet URL\"),\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Top Terms (TF-IDF Score)\"),\n",
        "            gr.Textbox(label=\"Generated Summary\"),\n",
        "            gr.Textbox(label=\"Generated Conclusion\")\n",
        "        ],\n",
        "        title=\"Google Sheet Title Analysis\",\n",
        "        description=\"Enter a Google Sheet URL containing titles (in the first column) to get top terms, a summary, and a conclusion.\"\n",
        "    )\n",
        "\n",
        "    # Launch the interface\n",
        "    interface.launch(debug=True)\n",
        "else:\n",
        "    print(\"Gradio interface not launched due to Google Colab authentication failure.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Colab authentication successful.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e191ff77d6ca8743d3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e191ff77d6ca8743d3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 254.74ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e191ff77d6ca8743d3.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7e576af7",
        "outputId": "b52b874c-99ea-40c5-a1a4-72e5f6cb982c"
      },
      "source": [
        "# Re-run the cell to launch the Gradio interface after setting the API key\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "\n",
        "# Authenticate with Google Colab (This part needs to be handled outside the function\n",
        "# or in a way compatible with Gradio. For Colab environment, it's typically done once.)\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    creds, project = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    print(\"Google Colab authentication successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Google Colab authentication failed: {e}\")\n",
        "    gc = None # Set gc to None if authentication fails\n",
        "\n",
        "# Get the Gemini API key outside the Gradio function\n",
        "print(\"Attempting to get Gemini API key outside the function...\") # Debug print\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_SERVICE_ACCOUNT_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"Gemini API key not found in user secrets. Gemini API functionality will be disabled.\") # Debug print\n",
        "    # You might want to raise an error or handle this more gracefully in a real application\n",
        "    # For this example, we'll allow the Gradio interface to launch but with a message\n",
        "    # indicating that the Gemini API key is missing.\n",
        "else:\n",
        "    print(\"Gemini API key successfully retrieved.\") # Debug print\n",
        "    try:\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        print(\"Gemini API configured successfully.\") # Debug print\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API with retrieved key: {e}\") # Debug print\n",
        "        # Handle potential API key configuration errors\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Segments Chinese text and removes stop words.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    seg_list = jieba.cut(text, cut_all=False)\n",
        "    # Add common stop words, including punctuation that might not be removed by default\n",
        "    stop_words = set([\n",
        "        '的', '是', '在', '我', '有', '了', '和', '就', '不', '人', '都', '一', '也', '很', '會', '你', '到', '想', '知', '更', '再', '沒', '看', '來', '要', '可', '行', '能', '說', '個', '這', '那', '用', '只', '比', '些', '從', '去', '先', '把', '給', '對',\n",
        "        ':', '：', ',', '，', '.', '。', '!', '！', '?', '？', ';', '；', '\"', '“', '”', \"'\", '‘', '’', '(', '（', ')', '）', '[', '【', ']', '】', '{', '}', '<', '>', '/', '\\\\', '|', '@', '#', '$', '%', '^', '&', '*', '_', '-', '+', '=', '~', '`', '...', '....', '.....', '......',\n",
        "        '死神', 'BLEACH', 'bleach', '《', '》' # Added '《' and '》'\n",
        "    ]) # Extended stop words including '死神', 'BLEACH', and punctuation\n",
        "    return \" \".join([word for word in seg_list if word not in stop_words and word.strip()])\n",
        "\n",
        "\n",
        "def run_analysis(sheet_url):\n",
        "    \"\"\"\n",
        "    Reads data from Google Sheet, performs TF-IDF, finds top terms,\n",
        "    generates summary and conclusion, and writes to the sheet.\n",
        "    \"\"\"\n",
        "    print(f\"Starting analysis for URL: {sheet_url}\") # Debug print\n",
        "\n",
        "    if gc is None:\n",
        "        print(\"Google Sheet access not authorized.\") # Debug print\n",
        "        return \"Google Sheet access not authorized. Please ensure authentication is set up correctly.\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        print(\"Attempting to open Google Sheet...\") # Debug print\n",
        "        sh = gc.open_by_url(sheet_url)\n",
        "        worksheet = sh.sheet1\n",
        "        print(\"Successfully opened Google Sheet.\") # Debug print\n",
        "        sheet_data = worksheet.get_all_values()\n",
        "        if not sheet_data:\n",
        "            print(\"No data found in the Google Sheet.\") # Debug print\n",
        "            return \"No data found in the Google Sheet.\", \"\", \"\"\n",
        "        print(f\"Read {len(sheet_data)} rows from Google Sheet.\") # Debug print\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading from Google Sheet: {e}\") # Debug print\n",
        "        return f\"Error reading from Google Sheet: {e}\", \"\", \"\"\n",
        "\n",
        "    # Assuming the data is in the first column after headers, if any\n",
        "    # Let's assume the first row is header and actual titles start from the second row\n",
        "    if len(sheet_data) > 1:\n",
        "        titles = [row[0] for row in sheet_data[1:] if row and len(row) > 0] # Extract titles from the first column, skip empty rows\n",
        "        print(f\"Extracted {len(titles)} titles for processing.\") # Debug print\n",
        "    else:\n",
        "         print(\"No title data found after header row.\") # Debug print\n",
        "         return \"No title data found after header row.\", \"\", \"\"\n",
        "\n",
        "\n",
        "    # Preprocess titles\n",
        "    print(\"Starting text preprocessing...\") # Debug print\n",
        "    try:\n",
        "        processed_titles = [preprocess_text(title) for title in titles]\n",
        "        processed_titles_joined = [title for title in processed_titles if title.strip()] # Filter out empty strings after preprocessing\n",
        "        print(f\"Finished text preprocessing. {len(processed_titles_joined)} titles remaining after filtering.\") # Debug print\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text preprocessing: {e}\") # Debug print\n",
        "        return f\"Error during text preprocessing: {e}\", \"\", \"\"\n",
        "\n",
        "\n",
        "    if not processed_titles_joined:\n",
        "         print(\"No valid text found after preprocessing.\") # Debug print\n",
        "         return \"No valid text found after preprocessing.\", \"\", \"\"\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    print(\"Calculating TF-IDF...\") # Debug print\n",
        "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, token_pattern=None)\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(processed_titles_joined)\n",
        "        print(\"TF-IDF calculation successful.\") # Debug print\n",
        "    except ValueError as e:\n",
        "        print(f\"Error calculating TF-IDF. Ensure there is enough text data to build vocabulary: {e}\") # Debug print\n",
        "        return f\"Error calculating TF-IDF. Ensure there is enough text data to build vocabulary: {e}\", \"\", \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error during TF-IDF calculation: {e}\") # Debug print\n",
        "        return f\"Unexpected error during TF-IDF calculation: {e}\", \"\", \"\"\n",
        "\n",
        "\n",
        "    # Find top N terms\n",
        "    print(\"Finding top N terms...\") # Debug print\n",
        "    try:\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        term_tfidf_sums = np.sum(tfidf_matrix, axis=0)\n",
        "        term_tfidf_sums = np.asarray(term_tfidf_sums).flatten()\n",
        "        term_importance = dict(zip(feature_names, term_tfidf_sums))\n",
        "        sorted_terms = sorted(term_importance.items(), key=lambda item: item[1], reverse=True)\n",
        "        N = 10  # Define number of top terms\n",
        "        top_n_terms = sorted_terms[:N]\n",
        "        top_terms_list = [term for term, score in top_n_terms]\n",
        "        print(f\"Found top {N} terms.\") # Debug print\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding top terms: {e}\") # Debug print\n",
        "        return f\"Error finding top terms: {e}\", \"\", \"\"\n",
        "\n",
        "\n",
        "    # Write to TF-IDF_Stats worksheet\n",
        "    print(\"Attempting to write to TF-IDF_Stats worksheet...\") # Debug print\n",
        "    try:\n",
        "        try:\n",
        "            stats_worksheet = sh.worksheet(\"TF-IDF_Stats\")\n",
        "            print(\"Opened existing 'TF-IDF_Stats' worksheet.\") # Debug print\n",
        "        except gspread.WorksheetNotFound:\n",
        "            stats_worksheet = sh.add_worksheet(title=\"TF-IDF_Stats\", rows=\"100\", cols=\"20\")\n",
        "            print(\"Created new 'TF-IDF_Stats' worksheet.\") # Debug print\n",
        "\n",
        "        stats_worksheet.clear()\n",
        "        print(\"'TF-IDF_Stats' worksheet cleared.\") # Debug print\n",
        "        header = ['Term', 'Total TF-IDF Score']\n",
        "        stats_worksheet.append_row(header)\n",
        "        print(\"Header row written.\") # Debug print\n",
        "        for term, score in top_n_terms:\n",
        "            stats_worksheet.append_row([term, score])\n",
        "        print(\"Top N terms and TF-IDF scores successfully written to Google Sheet.\") # Debug print\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to TF-IDF_Stats worksheet: {e}\") # Debug print\n",
        "        # Print error but don't stop the process\n",
        "\n",
        "\n",
        "    # Configure the Gemini API key\n",
        "    # It's assumed a standard API key is stored as a user secret in Colab under 'GOOGLE_SERVICE_ACCOUNT_KEY'\n",
        "    print(\"Attempting to get Gemini API key...\") # Debug print\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_SERVICE_ACCOUNT_KEY')\n",
        "    if not GOOGLE_API_KEY:\n",
        "        print(\"Gemini API key not found.\") # Debug print\n",
        "        generated_summary = \"Gemini API key not found. Cannot generate summary and conclusion.\"\n",
        "        generated_conclusion = \"Gemini API key not found. Cannot generate summary and conclusion.\"\n",
        "    else:\n",
        "        try:\n",
        "            print(\"Configuring Gemini API...\") # Debug print\n",
        "            genai.configure(api_key=GOOGLE_API_KEY)\n",
        "            model = genai.GenerativeModel('models/gemini-2.5-pro-preview-03-25')\n",
        "            print(\"Gemini API configured.\") # Debug print\n",
        "\n",
        "            # Prepare the prompt for Gemini\n",
        "            prompt_text = f\"\"\"\n",
        "            Based on the following list of titles from a forum and the top {N} most important terms found within them, provide a summary and a conclusion.\n",
        "\n",
        "            Titles (first 10 examples):\n",
        "            {titles[:10]}\n",
        "\n",
        "            Top {N} most important terms based on TF-IDF:\n",
        "            {top_n_terms}\n",
        "\n",
        "            Please generate:\n",
        "            1. Five insightful bullet points summarizing key observations about the topics and trends discussed in the titles based on the provided data.\n",
        "            2. A conclusion of approximately 120 words that synthesizes the main focus areas and overall sentiment (if discernible) of the forum content, referencing the important terms.\n",
        "            \"\"\"\n",
        "            print(\"Sending prompt to Gemini API...\") # Debug print\n",
        "            response = model.generate_content(prompt_text)\n",
        "            generated_text = response.text\n",
        "            print(\"Received response from Gemini API.\") # Debug print\n",
        "\n",
        "\n",
        "            # Basic parsing of the generated text\n",
        "            summary_heading = \"1.\"\n",
        "            conclusion_heading = \"2.\"\n",
        "\n",
        "            if summary_heading in generated_text and conclusion_heading in generated_text:\n",
        "                summary_start = generated_text.find(summary_heading)\n",
        "                conclusion_start = generated_text.find(conclusion_heading)\n",
        "\n",
        "                summary = generated_text[summary_start:conclusion_start].strip()\n",
        "                conclusion = generated_text[conclusion_start:].strip()\n",
        "            else:\n",
        "                 # Fallback parsing if headings aren't found as expected\n",
        "                print(\"Warning: Could not find expected headings in generated text. Attempting simple split.\") # Debug print\n",
        "                lines = generated_text.split('\\n')\n",
        "                summary_lines = []\n",
        "                conclusion_lines = []\n",
        "                is_conclusion = False\n",
        "                for line in lines:\n",
        "                    if \"conclusion\" in line.lower() or \"synthesize\" in line.lower() or \"總結\" in line:\n",
        "                         is_conclusion = True\n",
        "                    if not is_conclusion:\n",
        "                        summary_lines.append(line)\n",
        "                    else:\n",
        "                         conclusion_lines.append(line)\n",
        "                summary = \"\\n\".join(summary_lines).strip()\n",
        "                conclusion = \"\\n\".join(conclusion_lines).strip()\n",
        "\n",
        "\n",
        "            generated_summary = summary\n",
        "            generated_conclusion = conclusion\n",
        "            print(\"Successfully generated summary and conclusion.\") # Debug print\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating summary/conclusion with Gemini API: {e}\") # Debug print\n",
        "            generated_summary = f\"Error generating summary/conclusion with Gemini API: {e}\"\n",
        "            generated_conclusion = f\"Error generating summary/conclusion with Gemini API: {e}\"\n",
        "\n",
        "\n",
        "    # Format top terms for display\n",
        "    top_terms_display = \"\\n\".join([f\"{term}: {score:.4f}\" for term, score in top_n_terms])\n",
        "    print(\"Formatted top terms for display.\") # Debug print\n",
        "\n",
        "    print(\"Analysis complete. Returning results.\") # Debug print\n",
        "    return top_terms_display, generated_summary, generated_conclusion\n",
        "\n",
        "# Create the Gradio interface\n",
        "if gc is not None: # Only create interface if authentication was successful\n",
        "    print(\"Creating Gradio interface...\") # Debug print\n",
        "    interface = gr.Interface(\n",
        "        fn=run_analysis,\n",
        "        inputs=gr.Textbox(label=\"Enter Google Sheet URL\"),\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Top Terms (TF-IDF Score)\"),\n",
        "            gr.Textbox(label=\"Generated Summary\"),\n",
        "            gr.Textbox(label=\"Generated Conclusion\")\n",
        "        ],\n",
        "        title=\"Google Sheet Title Analysis\",\n",
        "        description=\"Enter a Google Sheet URL containing titles (in the first column) to get top terms, a summary, and a conclusion.\"\n",
        "    )\n",
        "\n",
        "    # Launch the interface\n",
        "    print(\"Launching Gradio interface...\") # Debug print\n",
        "    interface.launch(debug=True)\n",
        "else:\n",
        "    print(\"Gradio interface not launched due to Google Colab authentication failure.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Colab authentication successful.\n",
            "Attempting to get Gemini API key outside the function...\n",
            "Gemini API key successfully retrieved.\n",
            "Gemini API configured successfully.\n",
            "Creating Gradio interface...\n",
            "Launching Gradio interface...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://faa7eed7fda996a3b8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://faa7eed7fda996a3b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting analysis for URL: https://docs.google.com/spreadsheets/d/1GTsjiStggnSWJRGtwC0JRngF4wQAxvlyiGT8xFqSvNA/edit?usp=sharing\n",
            "Attempting to open Google Sheet...\n",
            "Successfully opened Google Sheet.\n",
            "Read 990 rows from Google Sheet.\n",
            "Extracted 989 titles for processing.\n",
            "Starting text preprocessing...\n",
            "Finished text preprocessing. 989 titles remaining after filtering.\n",
            "Calculating TF-IDF...\n",
            "TF-IDF calculation successful.\n",
            "Finding top N terms...\n",
            "Found top 10 terms.\n",
            "Attempting to write to TF-IDF_Stats worksheet...\n",
            "Opened existing 'TF-IDF_Stats' worksheet.\n",
            "'TF-IDF_Stats' worksheet cleared.\n",
            "Header row written.\n",
            "Top N terms and TF-IDF scores successfully written to Google Sheet.\n",
            "Attempting to get Gemini API key...\n",
            "Configuring Gemini API...\n",
            "Gemini API configured.\n",
            "Sending prompt to Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 304.50ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating summary/conclusion with Gemini API: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0\n",
            "Please retry in 27.889433403s.\n",
            "Formatted top terms for display.\n",
            "Analysis complete. Returning results.\n"
          ]
        }
      ]
    }
  ]
}